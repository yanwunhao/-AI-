## 課題一： Multi-head Attentionの出力次元数を計算する

長さ10の入力シーケンスがあり、各トークンの埋め込み（Embeddings）次元が64であるとします。8つのアテンションヘッドを使用する場合、マルチヘッドアテンションの出力次元を計算してください。

ヒント：

マルチヘッドアテンションでは、入力の埋め込み次元がヘッド数で分割されます。

各ヘッドは独立して注意計算を行います。

最終的に、全てのヘッドの出力が結合されます。

## 課題二： レイヤー正規化（Layer Normalization）関数を実装する

簡単なレイヤー正規化（Layer Normalization）関数を実装してください。

レイヤー正規化はTransformerの重要なコンポーネントで、各層の入力を正規化するために使用されます。アルゴリズムのステップは以下の通りです：

a) 入力テンソルの特徴次元に沿って平均を計算する

b) 入力テンソルの特徴次元に沿って分散を計算する

c) 計算された平均と分散を使用して入力を正規化する

この関数を実装し、簡単なテストケース(x=[1,2,3,4,5])を作成してください。

## チャレンジ：位置エンコーディング（Positional Encoding）関数を実装する

簡単な位置エンコーディング（Positional Encoding）関数を実装してください。

位置エンコーディングはTransformerで各位置に対して一意のエンコーディングを生成するために使用され、モデルがシーケンス内の位置情報を利用できるようにします。アルゴリズムのステップは以下の通りです：

a) 各位置に対して、異なる周波数の正弦波と余弦波の系列を生成する

b) 偶数インデックスには正弦関数を、奇数インデックスには余弦関数を使用する

c) 指数関数的に減衰する周波数を使用して、各位置のエンコーディングが一意であることを確認する

この関数を実装し、結果を可視化するための簡単なテストケース(max_seq_len=100, d_model=64)を作成してください。
